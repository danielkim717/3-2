{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielkim717/3-2/blob/main/%EC%98%88%EB%B9%84%EC%BA%A1%EC%8A%A4%ED%86%A4_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas matplotlib scikit-learn torch torchvision"
      ],
      "metadata": {
        "id": "DU3zFmniUbfY",
        "outputId": "2b8b6fa6-2f76-4706-efeb-719fa6a715d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # 파일 선택 → WM811K.pkl 업로드"
      ],
      "metadata": {
        "id": "DQWUcI_dUp4T",
        "outputId": "e4c2e540-6f6b-45d5-e472-fb8f7a9e5cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 59
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7f0d34a4-fbfd-4b63-8fc0-a8b1e8d1be04\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7f0d34a4-fbfd-4b63-8fc0-a8b1e8d1be04\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로드 및 분할\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 업로드한 pkl 불러오기\n",
        "df = pd.read_pickle(\"/content/WM811K.pkl\")\n",
        "\n",
        "print(df.head())\n",
        "print(\"train/val/test 분포:\", df[\"trainTestLabel\"].value_counts())\n"
      ],
      "metadata": {
        "id": "Hi-GqvSIU3_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋&데이터로더\n",
        "\n",
        "# ===== Colab 전용: WM-811K DataFrame 버전 → Dataset/DataLoader 통합 셋업 =====\n",
        "# 1) GPU 확인\n",
        "import torch, warnings, sys, subprocess\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# 2) WM811K.pkl 업로드 (한 번만 업로드하면 /content/ 에 존재)\n",
        "from google.colab import files\n",
        "print(\"👇 WM811K.pkl 파일을 선택해서 업로드하세요\")\n",
        "uploaded = files.upload()  # 파일 선택 창 뜸\n",
        "PKL_PATH = \"/content/WM811K.pkl\"  # 업로드한 파일명이 다르면 여기를 바꾸세요\n",
        "\n",
        "# 3) 로드 & 정규화(라벨/스플릿이 배열/대소문자 섞여도 안전)\n",
        "df = pd.read_pickle(PKL_PATH)\n",
        "print(\"DataFrame shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "def to_scalar_str(v):\n",
        "    if isinstance(v, (list, tuple, np.ndarray)):\n",
        "        a = np.array(v)\n",
        "        if a.size == 1:\n",
        "            try: v = a.item()\n",
        "            except Exception: v = a.reshape(-1)[0]\n",
        "        else:\n",
        "            v = \"|\".join(map(str, a.reshape(-1).tolist()))\n",
        "    return str(v)\n",
        "\n",
        "# 필수 컬럼 정규화\n",
        "assert \"waferMap\" in df.columns, \"waferMap 컬럼이 필요합니다.\"\n",
        "assert \"failureType\" in df.columns, \"failureType 컬럼이 필요합니다.\"\n",
        "df[\"failureType\"] = df[\"failureType\"].apply(to_scalar_str).str.strip()\n",
        "if \"trainTestLabel\" in df.columns:\n",
        "    df[\"trainTestLabel\"] = df[\"trainTestLabel\"].apply(to_scalar_str).str.strip()\n",
        "\n",
        "# 정상 라벨 표준화\n",
        "df[\"failureType\"] = df[\"failureType\"].replace(\n",
        "    {\"None\":\"none\",\"NONE\":\"none\",\"Null\":\"none\",\"null\":\"none\"}\n",
        ")\n",
        "\n",
        "# 스플릿 표준화 (training / test)\n",
        "def normalize_split(v: str) -> str:\n",
        "    lv = str(v).lower()\n",
        "    if \"train\" in lv: return \"training\"\n",
        "    if \"test\"  in lv: return \"test\"\n",
        "    if lv in {\"0\",\"1\"}: return \"training\" if lv==\"0\" else \"test\"\n",
        "    return lv\n",
        "if \"trainTestLabel\" in df.columns:\n",
        "    df[\"trainTestLabel\"] = df[\"trainTestLabel\"].apply(normalize_split)\n",
        "\n",
        "print(\"Unique failureType (sample 20):\", sorted(df[\"failureType\"].unique())[:20])\n",
        "if \"trainTestLabel\" in df.columns:\n",
        "    print(\"Split uniques:\", sorted(df[\"trainTestLabel\"].unique()))\n",
        "\n",
        "# 4) 제공 스플릿 그대로 사용 + train→val 분할\n",
        "SEED = 42\n",
        "if \"trainTestLabel\" in df.columns:\n",
        "    df_train_all = df[df[\"trainTestLabel\"]==\"training\"].copy()\n",
        "    df_test      = df[df[\"trainTestLabel\"]==\"test\"].copy()\n",
        "    if len(df_train_all)==0:\n",
        "        raise RuntimeError(\"training 표기가 0개입니다. 위 출력의 Split uniques를 확인하고 normalize_split 로직을 조정하세요.\")\n",
        "    df_train, df_val = train_test_split(\n",
        "        df_train_all, test_size=0.15, random_state=SEED, stratify=df_train_all[\"failureType\"]\n",
        "    )\n",
        "else:\n",
        "    # 만약 스플릿이 없으면 8:1:1로 생성\n",
        "    tr_all, df_test = train_test_split(df, test_size=0.1, random_state=SEED, stratify=df[\"failureType\"])\n",
        "    df_train, df_val = train_test_split(tr_all, test_size=0.111, random_state=SEED, stratify=tr_all[\"failureType\"])  # 0.111*0.9 ≈ 0.1\n",
        "\n",
        "print(f\"[SPLIT] train={len(df_train)}, val={len(df_val)}, test={len(df_test)}\")\n",
        "\n",
        "# 5) 라벨 매핑\n",
        "classes = sorted(df[\"failureType\"].unique().tolist())\n",
        "label2idx = {c:i for i,c in enumerate(classes)}\n",
        "idx2label = {i:c for c,i in label2idx.items()}\n",
        "print(f\"#classes = {len(classes)}\")\n",
        "\n",
        "# 6) 전처리/증강 보조 함수\n",
        "def to_tensor_1ch(img):\n",
        "    \"\"\"\n",
        "    numpy/image-like → torch.FloatTensor [1,H,W], 값 0~1 정규화\n",
        "    \"\"\"\n",
        "    a = np.array(img)\n",
        "    if a.ndim == 3 and a.shape[-1] == 1:\n",
        "        a = a[...,0]\n",
        "    a = a.astype(np.float32)\n",
        "    # 0~1 스케일\n",
        "    amin, amax = float(a.min()), float(a.max())\n",
        "    a = (a - amin) / (amax - amin + 1e-8) if amax > amin else np.zeros_like(a, dtype=np.float32)\n",
        "    t = torch.from_numpy(a).float().unsqueeze(0)  # [1,H,W]\n",
        "    return t\n",
        "\n",
        "def resize_tensor_bilinear(t, size=128):\n",
        "    return F.interpolate(t.unsqueeze(0), size=(size, size), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "\n",
        "def random_geom_aug(t):\n",
        "    # 90° 회전 + 좌우/상하 flip (웨이퍼 대칭)\n",
        "    k = random.choice([0,1,2,3])       # 0,90,180,270\n",
        "    t = torch.rot90(t, k, dims=[1,2])\n",
        "    if random.random() < 0.5:\n",
        "        t = torch.flip(t, dims=[2])    # Hflip\n",
        "    if random.random() < 0.5:\n",
        "        t = torch.flip(t, dims=[1])    # Vflip\n",
        "    return t\n",
        "\n",
        "# 7) WaferDataset\n",
        "from typing import Dict\n",
        "class WaferDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, label2idx: Dict[str,int], train: bool = True, img_size: int = 128):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label2idx = label2idx\n",
        "        self.train = train\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        img = row[\"waferMap\"]\n",
        "        y   = self.label2idx[str(row[\"failureType\"])]\n",
        "        t = to_tensor_1ch(img)\n",
        "        t = resize_tensor_bilinear(t, self.img_size)\n",
        "        if self.train:\n",
        "            t = random_geom_aug(t)\n",
        "        return t, y\n",
        "\n",
        "# 8) DataLoader (+불균형 대응 WeightedRandomSampler)\n",
        "IMG_SIZE = 128\n",
        "BATCH   = 128\n",
        "\n",
        "train_ds = WaferDataset(df_train, label2idx, train=True,  img_size=IMG_SIZE)\n",
        "val_ds   = WaferDataset(df_val,   label2idx, train=False, img_size=IMG_SIZE)\n",
        "test_ds  = WaferDataset(df_test,  label2idx, train=False, img_size=IMG_SIZE)\n",
        "\n",
        "# 클래스 가중치 (반비례 가중) + 샘플러\n",
        "vc = df_train[\"failureType\"].value_counts()\n",
        "w = np.array([1.0 / max(vc.get(lbl,1), 1) for lbl in classes], dtype=np.float32)\n",
        "w = w / w.mean()\n",
        "class_weight = torch.tensor(w, dtype=torch.float32)\n",
        "\n",
        "sample_weights = df_train[\"failureType\"].map(lambda s: class_weight[label2idx[s]].item()).values\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,   num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False,   num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"[OK] DataLoaders ready.\")\n",
        "\n",
        "# 9) 빠른 무결성 체크(한 배치 꺼내 보기)\n",
        "imgs, labels = next(iter(train_loader))\n",
        "print(\"Batch image tensor shape:\", imgs.shape)   # (B,1,128,128)\n",
        "print(\"Batch labels shape:\", labels.shape)       # (B,)\n",
        "\n",
        "# 라벨 일부를 실제 이름으로 매핑해보기\n",
        "idx2label = {v:k for k,v in label2idx.items()}\n",
        "print(\"Sample labels:\", [idx2label[int(l)] for l in labels[:10]])\n",
        "\n",
        "# 10) (선택) 배치 시각화 12장\n",
        "def show_grid(tensor_batch, label_batch, idx2label, cols=6):\n",
        "    n = min(len(tensor_batch), cols*2)\n",
        "    rows = math.ceil(n/cols)\n",
        "    plt.figure(figsize=(cols*2, rows*2))\n",
        "    for i in range(n):\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        plt.imshow(tensor_batch[i,0].cpu().numpy(), origin=\"lower\", interpolation=\"nearest\")\n",
        "        plt.title(idx2label[int(label_batch[i])], fontsize=8)\n",
        "        plt.xticks([]); plt.yticks([])\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "show_grid(imgs, labels, idx2label, cols=6)\n"
      ],
      "metadata": {
        "id": "EcKFu57hVUyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN 모델 정의\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class WaferCNN(nn.Module):\n",
        "    def __init__(self, num_classes, input_size=(1, 32, 32)):\n",
        "        super(WaferCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # 동적으로 fc1 입력 차원 계산\n",
        "        dummy_input = torch.zeros(1, *input_size)\n",
        "        conv_output_size = self._get_conv_output_size(dummy_input)\n",
        "\n",
        "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def _get_conv_output_size(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        n_size = x.view(x.size(0), -1).size(1)\n",
        "        return n_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mFcvBo_ZU9O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 루프\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = WaferCNN(num_classes=len(label2idx)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):  # 에포크 수는 나중에 조절\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "Mum3scJxVLnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평가\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels, all_preds, target_names=list(label2idx.keys())))\n"
      ],
      "metadata": {
        "id": "sG5FE79NVOpr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}